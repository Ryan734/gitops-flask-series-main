# Deploying to multiple Kubernetes clusters via GitOps

In this part we will explore how Flux can be used to deploy to multiple Kubernetes clusters via a single Git repository. There are various strategies that can be used for environment segregation but we will discuss some common scenarios.

## Setting up Part 7

Create a new directory called `part7` inside the `gitops_tutorial` directory and copy `test_end_to_end.py` from part 4. We will be running commands from `gitops_tutorial/part7` and `gitops-series` directories. 

## Motivations for multiple clusters

A common workflow pattern for application development is to first deploy to a development environment, then go to staging then eventually production. Each environment can have a separate cluster to avoid mingling or to stop noisy neighbors. Another example can be of deploying the same Flask application in different geographical regions: as the client gets closer to the physical server running the application, the network latency decreases and the user's experience improves. More about environments [here](https://www.gratasoftware.com/what-is-each-server-for-development-test-uat-or-staging-demo-and-production/). In this section we will create 3 environments, development, staging and production and setup CD pipelines using Flux v2.

## Approaching the problem

There are many ways of handling this via GitOps but we will discuss two. First approach is to simply have one single repository and modify the Kustomization CRD to only look at say the `production` or `staging` git path when setting up the CRD. The second approach is to simply have multiple repositories and throw everything in there related to that environment. Both have their pros and cons, let's compare both approaches.

| Approach             | Central Source of Truth | Same Permissions | Shared CD   | 
| ---------------------| ------------------------| -----------------| ----------- |
| Single Repository    | &check;                 | &check;          | &check;     |
| Multiple Repositories| &cross;                 | &cross;          | &cross;     |    


 As we can see in the table above that with the single repository approach we have a central source of truth, meaning that one repository represents everything we have running in all of our environments at a given time. With the second approach we need to look at multiple repositories to piece together what is running in an environment. However security wise we can set distinct policies and permissions on each of the individual repositories, some maybe more important than others. Having say a higher number of approvers for production pull requests compared to development is a common pattern. Another risk posed by the single repository approach is that all environments share a single `flux-system` Namespace hence if say the one of the controllers is down due to a bad commit or upgrade all the environments can potentially inherit this behavior.

In this part we are going to demonstrate the first approach and we can deploy and test our Flask application on multiple clusters just by committing to our git repository.

## Setting up multiple clusters

In the previous parts we created `cluster1`, if you have deleted it, please recreate it based on part 4 as we need the ingresses working. Don't remember if you deleted the cluster? Run the following command to confirm:

```
$ kind get clusters
```

Each cluster requires a few GBs of space, I recommend to have at least 10GB free for the new clusters. We are now going to create a `cluster2` and `cluster3`.

```bash
$ cat <<EOF | kind create cluster --name cluster2 --config=-
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 81
    hostPort: 81
    protocol: TCP
EOF
Creating cluster "cluster1" ...
 ✓ Ensuring node image (kindest/node:v1.19.1) 🖼
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
Set kubectl context to "kind-cluster2"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster2

Thanks for using kind! 😊

# create new cluster 3
$ cat <<EOF | kind create cluster --name cluster3 --config=-
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 82
    hostPort: 82
    protocol: TCP
EOF
Creating cluster "cluster1" ...
 ✓ Ensuring node image (kindest/node:v1.19.1) 🖼
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
Set kubectl context to "kind-cluster3"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster3

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂
```

Note that `cluster2` uses port `81` for the HTTP port mappings and `cluster3` uses port `82` to avoid clashing with `cluster1`.

Run the `kind get clusters` command to confirm that all three clusters are running:

```
$ kind get clusters
cluster1
cluster2
cluster3
```

## Running smoke tests

We have created two new clusters. Let's run our smoke tests against them to see if they are fit for use. Before we can run our tests we need to be able to pass a different context as it is currently hard coded. We need to make the following changes.

```python
# gitops_tutorial/part7/tests/conftest.py

from pytest import fixture


def pytest_addoption(parser):
    parser.addoption(
        "--endpoint",
        action="store"
    )

    # new
    parser.addoption(
        "--context",
        action="store"
    )


@fixture()
def endpoint(request):
    return request.config.getoption("--endpoint")


# new
def pytest_configure(config):
    global option
    option = config.option
```

```python
# gitops_tutorial/part7/tests/test_smoke.py

from kubernetes import client, config
import pytest
import yaml
from conftest import option  # new

config.load_kube_config(context=option.context)  # changed


class TestSmoke:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.networking_v1beta1 = client.NetworkingV1beta1Api()

    def test_create_namespace(self):
        with open("namespace.yaml") as f:
            namespace = yaml.safe_load(f)
        response = self.core_v1.create_namespace(body=namespace)
        result = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == result.metadata.name

    def test_create_deployment(self):
        with open("deployment.yaml") as f:
            deployment = yaml.safe_load(f)
        response = self.apps_v1.create_namespaced_deployment(body=deployment, namespace="test-namespace")
        result = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def test_create_service(self):
        with open("service.yaml") as f:
            service = yaml.safe_load(f)
        response = self.core_v1.create_namespaced_service(body=service, namespace="test-namespace")
        result = self.core_v1.read_namespaced_service(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def test_create_ingress(self):
        with open("ingress.yaml") as f:
            ingress = yaml.safe_load(f)
        response = self.networking_v1beta1.create_namespaced_ingress(body=ingress, namespace="test-namespace")
        result = self.networking_v1beta1.read_namespaced_ingress(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def teardown_class(self):
        self.core_v1.delete_namespace(name="test-namespace")
```

Let's run our smoke tests.

```bash
(env)$ pytest tests/test_smoke.py --context kind-cluster2
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 4 items

test_smoke.py ....                                                                                                                                              [100%]

========================================================================== 4 passed in 1.11s ==========================================================================

(env)$ pytest tests/test_smoke.py --context kind-cluster3
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 4 items

test_smoke.py ....                                                                                                                                              [100%]

========================================================================== 4 passed in 0.78s ==========================================================================
```

The tests pass! If the tests are failing here then debugging might be needed to understand why the `kind` clusters did not come up properly.

## Deploying Flux and multiple clusters

As mentioned earlier we are going to have a single repository and reference the `path` value to reference our environments. Let's deploy all of our Flux v2 applications to the new clusters.

```bash
$ export GITHUB_USER=<your-username>
$ export GITHUB_TOKEN=<token from https://github.com/settings/tokens>

# Recreate Flux in cluster1, safe to run on an existing Flux v2 installation
# Install Flux v2
$ flux bootstrap github \
  --owner=$GITHUB_USER \
  --repository=fleet-infra \
  --branch=main \
  --path=./clusters/kind/cluster1 \
  --context=kind-cluster1 \
  --personal

# Install flux in cluster2
$ flux bootstrap github \
  --owner=$GITHUB_USER \
  --repository=fleet-infra \
  --branch=main \
  --path=./clusters/kind/cluster2 \
  --context=kind-cluster2 \
  --personal

# Create flux in cluster3
$ flux bootstrap github \
  --owner=$GITHUB_USER \
  --repository=fleet-infra \
  --branch=main \
  --path=./clusters/kind/cluster3 \
  --context=kind-cluster3 \
  --personal
```

We should have all the Pods in the flux-system namespaces running on all three clusters, pleas confirm by running `kubectl get pods -n flux-system`, all Pods should be running. Take note that each cluster is looking at a different git path: `cluster1` is looking at `./clusters/kind/cluster1`, `cluster2` at `./clusters/kind/cluster2`, and `cluster3` at `./clusters/kind/cluster3` in this context. Let's assign an environment to our clusters.

| Cluster  | Environment       | 
| ---------| ------------------|
| cluster1 | development       | 
| cluster2 | staging           |    
| cluster3 | production        |


We can now follow a workflow where we can experiment in the development environment, then deploy to staging then eventually production. This way if we can squash and bugs in our CD pipeline or application before they make it to production. Let's go ahead and create the GitRepository CRD on all 3 clusters. If you created the Bucket CRD from the previous part then please delete the `flask` Bucket CRD in cluster1 as we are going to use the GitRepository CRD going forward.

```bash
# delete bucket CRD from part 6
$ kubectl delete bucket flask -n flux-system --context kind-cluster1
```

```yaml
# Apply on cluster2 and cluster3
# gitops_tutorial/part7/gitrepository.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: gitops-series
  namespace: flux-system
spec:
  interval: 30s
  ref:
    branch: main
  # replace with your username  
  url: https://github.com/paktek123/gitops-series
```

Take note of the branch, it is possible to setup a Git workflow such that each environment lives in a different git branch. This is a common workflow and can segregate the environments further in a single repository. We will only reference the main branch on all clusters for brevity purposes but this something to consider when creating your own environments. Before we setup the sync between our `gitops-series` repository and the three clusters, we'll add some manifests and setup our git repository. 

```sh
$ cd gitops-series
```

Then, run the following commands:

```sh
$ mkdir -p development/flask staging/flask production/flask
$ cp *.yaml development/flask 
$ cp *.yaml staging/flask
$ cp *.yaml production/flask
$ rm *.yaml
$ git add .
$ git commit -m 'separate into different environments'
$ git push
```

Let's create the kustomization resource to enable the CD pipeline.

```yaml
# create for all 3 clusters and kubectl apply on the relevant cluster

# gitops_tutorial/part7/kustomization_flask_application_development.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flask-application
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./development/
  prune: true
  sourceRef:
    kind: GitRepository
    name: gitops-series
  validation: client

# gitops_tutorial/part7/kustomization_flask_application_staging.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flask-application
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./staging/
  prune: true
  sourceRef:
    kind: GitRepository
    name: gitops-series
  validation: client

# gitops_tutorial/part7/kustomization_flask_application_production.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flask-application
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./production/
  prune: true
  sourceRef:
    kind: GitRepository
    name: gitops-series
  validation: client
```

Take note in the above we need to create the Kustomization CRD but change the `path` to the relevant cluster based on the mappings table we created above. Don't forget to run the following to import docker image into your new clusters:

```bash
$ kind load docker-image flask-test-gitops:latest --name cluster1
$ kind load docker-image flask-test-gitops:latest --name cluster2
$ kind load docker-image flask-test-gitops:latest --name cluster3
```

## Setting up NGINX Ingress controller and Kustomizing our deployments

For Ingress to work as expected, we'll need to deploy the NGINX Ingress Controller to all three clusters alongside our Flask application too.

Change directory to your `gitops-series` project.

```sh

# add NGINX Ingress
$ mkdir development/ingress-nginx staging/ingress-nginx production/ingress-nginx
$ curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml -o development/ingress-nginx/ingress-nginx.yaml
$ curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml -o staging/ingress-nginx/ingress-nginx.yaml
$ curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml -o production/ingress-nginx/ingress-nginx.yaml
$ git add development/ staging/ production/
$ git commit -m 'adding ingress nginx'
$ git push
```

We seem to copying the exact same manifests to all of the clusters with NGINX Ingress and our sample application, this can cause repetitive code that is bulky and hard to manage. To deploy we can simply create a Kustomization CRD and get everything working or ee can use kustomize to create one base manifest then reference it in our NGINX Ingress Deployment. Let's start off by creating a `base` directory on the root of our `gitops-series` repository, then create a `flask` and `ingress-nginx` directory inside.

```bash
$ mkdir -p base/flask
$ mkdir -p base/ingress-nginx
```

The directory structure of `gitops-series` repository should look like below:
```bash
├── README.md
├── base
│   ├── flask
│   └── ingress-nginx
├── development
│   ├── deployment.yaml
│   ├── ingress-nginx
│   │   └── ingress-nginx.yaml
│   ├── ingress.yaml
│   ├── kustomization.yaml
│   ├── namespace.yaml
│   └── service.yaml
├── production
│   ├── deployment.yaml
│   ├── ingress-nginx
│   │   └── ingress-nginx.yaml
│   ├── ingress.yaml
│   ├── kustomization.yaml
│   ├── namespace.yaml
│   └── service.yaml
└── staging
    ├── deployment.yaml
    ├── ingress-nginx
    │   └── ingress-nginx.yaml
    ├── ingress.yaml
    ├── kustomization.yaml
    ├── namespace.yaml
    └── service.yaml
```
Inside of the `base/flask` directory we will put a copy of our application and a `kustomization.yaml`.

```bash
# since all the manifests are the same, we can take the development one
$ cp development/*.yaml base/flask/
```

Now let's do something similar to NGINX Ingress.

```bash
$ cp -r development/ingress-nginx base/
```

```yaml
# gitops-series/base/ingress-nginx/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ingress-nginx.yaml
```
Now we need to create a `kustomization.yaml` on the root of the base directory.

```yaml
# gitops-series/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./flask/
  - ./ingress-nginx/
```

Our base directory in `gitops-series` should look like this now.

```bash
├── base
│   ├── flask
│   │   ├── deployment.yaml
│   │   ├── ingress.yaml
│   │   ├── kustomization.yaml
│   │   ├── namespace.yaml
│   │   └── service.yaml
│   ├── ingress-nginx
│   │   ├── ingress-nginx.yaml
│   │   └── kustomization.yaml
│   └── kustomization.yaml
```
What are we doing? Essentially kustomize allows us to reference other kustomization files that are embedded into child directories which inherit their content. Let's empty out development, staging and production directories since we don't need them anymore.

```bash
$ rm -r deployment/* staging/* production/*
```
Now we will put a new `kustomization.yaml` in all of them which will reference the base.

```yaml
# gitops-series/development/kustomization.yaml, gitops-series/staging/kustomization.yaml, gitops-series/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base/
```

The workflow works like this:
1. Our Kustomization CRD points at `./development/` and finds the `kustomization.yaml` there which references `../base/`
2. Kustomize looks at the `kustomization.yaml` in `base` which references the `kustomization.yaml` in `base/flask` and `base/ingress-nginx`.
3. The kustomization.yaml in `base/flask` and `base/ingress-nginx` points it to the relevant manifests that need to applied to the cluster.

Let's check if our CD pipeline worked.

## Testing if our GitOps deployment worked

We wrote a `test_gitops.py` test to see if our cluster is running the correct GitOps manifests back in part 6. We are going to modify these tests similar to how we modified our smoke tests earlier.

```py
# gitops_tutorial/part7/tests/test_gitops.py

from kubernetes import client, config
import pytest
import yaml
from conftest import option  # new

config.load_kube_config(context=option.context)  # changed


class TestGitOps:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.networking_v1beta1 = client.NetworkingV1beta1Api()

    def test_flask_application_namespace(self):
        namespace = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == namespace.metadata.name

    def test_flask_application_deployment(self):
        deployment = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == deployment.metadata.name
        assert "test-namespace" == deployment.metadata.namespace

    def test_flask_application_service(self):
        service = self.core_v1.read_namespaced_service(name="flask", namespace="test-namespace")
        assert "flask" == service.metadata.name
        assert "test-namespace" == service.metadata.namespace

    def test_flask_application_ingress(self):
        ingress = self.networking_v1beta1.read_namespaced_ingress(name="flask", namespace="test-namespace")
        assert "flask" == ingress.metadata.name
        assert "test-namespace" == ingress.metadata.namespace
```

In the above we can share the same `conftest.py` and import over the option to set the context we pass. Let's run our tests to confirm if everything deployed as expected.

```sh
(env)$ pytest tests/test_gitops.py --context kind-cluster1
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 4 items

test_gitops.py ....                                                                                                                                             [100%]

========================================================================== 4 passed in 0.57s ==========================================================================

(env)$ pytest tests/test_gitops.py --context kind-cluster2
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 4 items

test_gitops.py ....                                                                                                                                             [100%]

========================================================================== 4 passed in 0.94s ==========================================================================

(env)$ pytest tests/test_gitops.py --context kind-cluster3
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 4 items

test_gitops.py ....                                                                                                                                             [100%]

========================================================================== 4 passed in 0.58s ==========================================================================
```

The tests pass! This means that our Flux deployed as expected. Let's run our application end-to-end tests (setup in part 3) now to verify if everything is up and running.

```sh
(env)$ pytest tests/test_end_to_end.py --endpoint localhost:80
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 1 item

test_end_to_end.py .                                                                                                                                            [100%]

========================================================================== 1 passed in 0.02s ==========================================================================

(env)$ pytest tests/test_end_to_end.py --endpoint localhost:81
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 1 item

test_end_to_end.py F                                                                                                                                            [100%]

============================================================================== FAILURES ===============================================================================
_____________________________________________________________________________ test_index ______________________________________________________________________________

(env)$ pytest tests/test_end_to_end.py --endpoint localhost:82
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 1 item

test_end_to_end.py F                                                                                                                                            [100%]

============================================================================== FAILURES ===============================================================================
_____________________________________________________________________________ test_index ______________________________________________________________________________
```

We are getting 2 failures in cluster 2 and cluster 3. Let's determine the cause by looking at the Ingress.

```sh
$ kubectl get ing -n test-namespace --context kind-cluster1
NAME    CLASS    HOSTS   ADDRESS     PORTS   AGE
flask   <none>   *       localhost   80      92m
$ kubectl get ing -n test-namespace --context kind-cluster2
NAME    CLASS    HOSTS   ADDRESS     PORTS   AGE
flask   <none>   *       localhost   80      92m
$ kubectl get ing -n test-namespace --context kind-cluster3
NAME    CLASS    HOSTS   ADDRESS     PORTS   AGE
flask   <none>   *       localhost   80      92m
```

As we can see the Ingress is still listening on port 80 for cluster 2 and 3. When creating the `kind` cluster, we only exposed 81 and 82 for cluster 2 and 3 respectively. The port for NGINX controller is determined by the Service in the ingress-nginx Namespace.

```sh
$ kubectl get svc -n ingress-nginx --context kind-cluster1
NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.96.185.20   <none>        80:31278/TCP,443:30873/TCP   2d23h
ingress-nginx-controller-admission   ClusterIP   10.96.7.43     <none>        443/TCP                      2d23h

$ kubectl get svc -n ingress-nginx --context kind-cluster2
NAME                                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.96.6.61    <none>        80:30850/TCP,443:32717/TCP   93m
ingress-nginx-controller-admission   ClusterIP   10.96.5.119   <none>        443/TCP                      93m

$ kubectl get svc -n ingress-nginx --context kind-cluster3
NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.96.91.85    <none>        80:31243/TCP,443:30520/TCP   94m
ingress-nginx-controller-admission   ClusterIP   10.96.110.14   <none>        443/TCP                      94m
```

The ports are incorrect as we suspected due to the end to end test failures. Let's change this for cluster 2 and 3 or "staging" and "production". 

## Kustomize Strategic Merge with multiple environments

As mentioned in the previous sections we setup a base directory and all environments inherit from this but we have a scenario here where we need each environment to have unique parameters. In this scenario we can leverage what is known as a [Strategic Merge](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md). This works by creating a patch to the manifest which gets merged to override the base configuration. Let's go through an example of NGINX Ingress.

```yaml
# gitops-series/staging/kustomization.yaml, gitops-series/production/kustomization.yaml, gitops-series/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base/
patchesStrategicMerge:
  - patches/ingress-nginx-deployment.yaml
  - patches/ingress-nginx-service.yaml
```

We will now create a `patches` directory and this will hold our patches.

```yaml
# gitops-series/staging/patches/ingress-nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  template:
    spec:
      containers:
        - name: controller
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
              hostPort: 81
---
# gitops-series/staging/patches/ingress-nginx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 81
      protocol: TCP
      targetPort: http
---
# gitops-series/production/patches/ingress-nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  template:
    spec:
      containers:
        - name: controller
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
              hostPort: 82
---
# gitops-series/production/patches/ingress-nginx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 82
      protocol: TCP
      targetPort: http
```
The changes above only reference the relevant yaml that needs to be changed. In the above we are changing the `hostPort` of the container to 81/82 and changing the `NodePort` to 81/82. A NodePort Service listens on the same port of all Kubernetes workers, in our case we only have one Node per cluster but the same concept can be applied to a cluster with multiple Nodes. Before we commit the changes there is one more thing required. Since the patches are append only and replace, in this case duplicate ports will be added. Let's look at an example.

```yaml
# gitops-series/base/ingress-nginx/ingress-nginx.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
  labels:
    helm.sh/chart: ingress-nginx-3.23.0
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 0.44.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
#   - name: http   <------ added by our patch 
#     port: 80
#     protocol: TCP
#     targetPort: http
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/component: controller
```
In the above the commented lines will be added by our patch. This will cause an error since we now have duplicate port called "http". To fix this , in the base we will remove the http port entirely and it will get merged in by our patch instead. This means we will need to add a patch for development environment and remove the reference in base for the Service and the Deployment resources. Let's go ahead and do so. The final `ingress-nginx.yaml` will look like [this](https://github.com/paktek123/gitops-series/blob/main/base/ingress-nginx/ingress-nginx.yaml) and the development patch will look as follows:

```yaml
# gitops-series/development/patches/ingress-nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  template:
    spec:
      containers:
        - name: controller
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
              hostPort: 80
---
# gitops-series/development/patches/ingress-nginx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
---
# gitops-series/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base/
patchesStrategicMerge:
  - patches/ingress-nginx-deployment.yaml
  - patches/ingress-nginx-service.yaml
```

Let's commit the changes and push. 

```sh
$ kubectl get svc -n ingress-nginx --context kind-cluster1
NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.96.125.66   <none>        80:30284/TCP,443:31168/TCP   4h28m
ingress-nginx-controller-admission   ClusterIP   10.96.44.92    <none>        443/TCP                      4h28m
$ kubectl get svc -n ingress-nginx --context kind-cluster2
NAME                                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.96.6.61    <none>        81:30057/TCP,443:32717/TCP   105m
ingress-nginx-controller-admission   ClusterIP   10.96.5.119   <none>        443/TCP                      105m
$ kubectl get svc -n ingress-nginx --context kind-cluster3
NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.96.91.85    <none>        82:31040/TCP,443:30520/TCP   107m
ingress-nginx-controller-admission   ClusterIP   10.96.110.14   <none>        443/TCP                      107m
```

The NodePort has been updated but our Ingress will still show the old port 80. This is because Ingresses can only expose port 80 and/or 443, to expose any other ports we will need to use a NodePort, which we are using already. Let's run our tests again to see if they pass this time.

```sh
(env)$ pytest tests/test_end_to_end.py --endpoint localhost:80
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 1 item

test_end_to_end.py .                                                                                                                                            [100%]

========================================================================== 1 passed in 0.02s ==========================================================================

(env)$ pytest tests/test_end_to_end.py --endpoint localhost:81
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 1 item

test_end_to_end.py .                                                                                                                                            [100%]

========================================================================== 1 passed in 0.02s ==========================================================================

(env)$ pytest tests/test_end_to_end.py --endpoint localhost:82
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part7
collected 1 item

test_end_to_end.py .                                                                                                                                            [100%]

========================================================================== 1 passed in 0.02s ==========================================================================
```

## Directory structure

To see how the directory structure should look like, checkout the part7 branch on my [gitops-series](https://www.github.com/paktek123/gitops-series) git repository.

## Summary

In this part we created multiple `kind` clusters and deployed NGINX Ingress controller and our Flask application to all of them. To prevent repetition we made use of kustomize to create a base manifest and unique strategic merges for each environment. We ran smoke tests, GitOps tests verification tests and end to end tests to verify that all components are working as expected.

## Further reading

- NodePort services: https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0
- kubectl and multiple clusters: https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/
- Changing port on ingress: https://stackoverflow.com/questions/56243121/can-i-set-custom-ports-for-a-kubernetes-ingress-to-listen-on-besides-80-443
