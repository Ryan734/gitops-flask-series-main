# Kubernetes Deployment

In this chapter, you'll learn how to run a Flask application on Kubernetes using a Kubernetes Deployment.

## Kubernetes Concepts

Before diving in, let's look at some of the basic building blocks that you have to work with from the [Kubernetes API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/):

1. A **[Node](https://kubernetes.io/docs/concepts/architecture/nodes/)** is a worker machine provisioned to run Kubernetes. Each Node is managed by the Kubernetes master.
1. A **[Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/)** is a logical, tightly-coupled group of application containers that run on a Node. Containers in a Pod are deployed together and share resources (like data volumes and network addresses). Multiple Pods can run on a single Node.
1. A **[Service](https://kubernetes.io/docs/concepts/services-networking/service/)** is a logical set of Pods that perform a similar function. It enables load balancing and service discovery. It's an abstraction layer over the Pods; Pods are meant to be ephemeral while services are much more persistent.
1. **[Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)** are used to describe the desired state of Kubernetes. They dictate how Pods are created, deployed, and replicated.
1. **[Labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)** are key/value pairs that are attached to resources (like Pods) which are used to organize related resources. You can think of them like CSS selectors. For example:
    - *Environment* - `dev`, `test`, `prod`
    - *App version* - `beta`, `1.2.1`
    - *Type* - `client`, `server`, `db`
1. **[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)** is a set of routing rules used to control the external access to Services based on the request host or path.
1. **[Volumes](https://kubernetes.io/docs/concepts/storage/volumes/)** are used to persist data beyond the life of a container. They are especially important for stateful applications like Redis and Postgres.
    - A *[PersistentVolume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)* defines a storage volume independent of the normal Pod-lifecycle. It's managed outside of the particular Pod that it resides in.
    - A *[PersistentVolumeClaim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)* is a request to use the PersistentVolume by a user.

> For more, review the [Learn Kubernetes Basics](https://kubernetes.io/docs/tutorials/kubernetes-basics/) tutorial as well as the [Kubernetes Concepts](https://mherman.org/presentations/flask-kubernetes/#38) slides from the [Scaling Flask with Kubernetes](https://mherman.org/presentations/flask-kubernetes) talk.

## Deployment

Since we're taking a test-first approach, let's create our tests first.

First, define our Deployment config:

```yaml
# gitops-flask-kubernetes/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask
  namespace: test-namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flask
  template:
    metadata:
      labels:
        app: flask
    spec:
      containers:
      - name: flask
        image: flask-test-gitops:latest
        ports:
        - containerPort: 5000
```

Here, we used the `apps/v1` [API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/). In the `metadata` we defined the name of our Deployment and referenced the Namespace that we created in the previous chapter. We also defined two [replicas](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/), so that two Pods will be spun up, and the containers. In this case, we just have one container which will run the Flask application. Did you notice the `flask-test-gitops:latest` Docker image? It doesn't exist yet. Neither does the Flask app. Let's create them both now.


## Flask App

Remember: We're using Test-driven Development, so let's write tests to guide us as we build our Flask app.

Add the following code to a new file called *test_app.py* in the project root:

```python
# gitops-flask-kubernetes/tests/test_app.py

import pytest

from app import app


@pytest.fixture
def client():
    with app.test_client() as client:
        yield client


def test_index(client):
    result = client.get('/')
    assert b'hello' in result.data
```

Next, let's run the tests to ensure they fail:

```bash
(env)$ pytest tests/test_app.py
```

You should see:

```bash
    from app import app
E   ModuleNotFoundError: No module named 'app'
```

Create an *app.py* file:

```python
# gitops-flask-kubernetes/app.py

from flask import Flask

app = Flask(__name__)


@app.route('/')
def hello():
    return "hello"


if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

Run the tests again to ensure they now pass:

```bash
(env)$ touch tests/__init__.py # create an empty __init__.py to allow our test to import the app from our project root
(env)$ pytest tests/test_app.py
```

With the app done, let's turn to the Docker image.

Create a *Dockerfile*:

```Dockerfile
# gitops-flask-kubernetes/Dockerfile

FROM python:3.9-alpine
WORKDIR /app
RUN pip install --upgrade pip
RUN pip install "Flask==2.0.1"
COPY ./app.py .
CMD ["python", "/app/app.py"]
```

Here, we extended from a [Docker Alpine](https://hackernoon.com/you-should-use-alpine-linux-instead-of-ubuntu-yb193ujt) flavor of Python 3.9, installed Flask, copied over the *app.py* file, and ran the Flask dev server.

Build the image:

```bash
(env)$ docker build -t flask-test-gitops:latest .
```

Our docker image is ready. Now we can update our smoke tests to test the creation of a Deployment resource.

## Smoke Tests

Add Deployments to our smoke tests:

```python
# gitops-flask-kubernetes/tests/test_smoke.py


import yaml
from kubernetes import client, config

config.load_kube_config(context="kind-cluster1")


class TestSmoke:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api() # new

    def test_create_namespace(self):
        with open("namespace.yaml") as f:
            namespace = yaml.safe_load(f)
        response = self.core_v1.create_namespace(body=namespace)
        result = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == result.metadata.name

    # new
    def test_create_deployment(self):
        with open("deployment.yaml") as f:
            deployment = yaml.safe_load(f)
        response = self.apps_v1.create_namespaced_deployment(body=deployment, namespace="test-namespace")
        result = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def teardown_class(self):
        self.core_v1.delete_namespace(name="test-namespace")
```

We added a new `test_create_deployment()` test case that:

1. Loads our *deployment.yaml* file.
1. Invokes the API to create a namespaced Deployment.
1. Asserts that the metadata matches our expectations.

Also, note that we've added a new `self.apps_v1` attribute to our `setup_class()` method. This variable instantiates the [apps v1 API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/).

Run the tests:

```bash
$ pytest tests/test_smoke.py

============================== test session starts ==============================
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
rootdir: /Users/michael/repos/testdriven/gitops-flask-kubernetes
collected 2 items

test_smoke.py ..                                                          [100%]

=============================== 2 passed in 1.10s ===============================
```

Our tests now pass, the same principle applies from the last chapter where if we run too quickly we get an error that the Namespace is being deleted. We have verified that the Deployment gets created but how do we know that the Deployment is actually running? The answer is we need to execute tests against our application.

## Load Docker Image

Each Pod that is created from our Deployment has a state. We need our Pod to be in the `Running` state to be able to test against it. Let's create our Namespace and Deployment manually and check if the Pod is running.

```bash
# Switch the context to the kind cluster
$ kubectl config use-context kind-cluster1

# Apply our Namespace to the cluster
$ kubectl apply -f namespace.yaml
namespace/test-namespace created

# Apply our Deployment to the cluster
$ kubectl apply -f deployment.yaml
deployment.apps/flask created

$ kubectl get pods -n test-namespace
NAME                     READY   STATUS         RESTARTS   AGE
flask-6759dd97d8-69j4z   0/1     ErrImagePull   0          15s
flask-6759dd97d8-s42hh   0/1     ErrImagePull   0          15s
```

Remember the ReplicaSet we defined in the `deployment.yaml` file, specifically the `replicas: 2` line? Our Deployment created two containers running the same Flash application: `flask-6759dd97d8-69j4z` and `flask-6759dd97d8-s42hh`. (Your container name suffixes will differ.) According to the output, our containers are in the `ErrImagePull` state. Let's dig deeper into why we're getting this error. Replace the Pod name with the one running locally.

Run the following command, substituting your first container's name where appropriate:

```bash
$ kubectl describe pod flask-6759dd97d8-69j4z -n test-namespace

Name:         flask-6759dd97d8-69j4z
Namespace:    test-namespace
Priority:     0
Node:         cluster1-control-plane/172.18.0.2
Start Time:   Thu, 24 Dec 2020 20:54:19 +0000
...
...
...
Events:
  Type     Reason     Age                From                             Message
  ----     ------     ----               ----                             -------
  Normal   Scheduled  <unknown>                                           Successfully assigned test-namespace/flask-6759dd97d8-69j4z to cluster1-control-plane
  Normal   BackOff    27s                kubelet, cluster1-control-plane  Back-off pulling image "flask-test-gitops:latest"
  Warning  Failed     27s                kubelet, cluster1-control-plane  Error: ImagePullBackOff
  Normal   Pulling    13s (x2 over 31s)  kubelet, cluster1-control-plane  Pulling image "flask-test-gitops:latest"
  Warning  Failed     11s (x2 over 27s)  kubelet, cluster1-control-plane  Failed to pull image "flask-test-gitops:latest": rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/flask-test-gitops:latest": failed to resolve reference "docker.io/library/flask-test-gitops:latest": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     11s (x2 over 27s)  kubelet, cluster1-control-plane  Error: ErrImagePull
```

In the above output we are largely interested in the events near the bottom. We can see that our Kubernetes Node is trying to pull from `docker.io/library/flask-test-gitops:latest` but that is not the image we passed in our Deployment. To figure this out we have to understand how `kind` or Kubernetes in general works. When we built our Docker image, we built locally on *our* laptop / PC / Workstation. The Docker image does not exist on the Node and by default the Docker container runtime will pull from `docker.io`. Let's [load](https://kind.sigs.k8s.io/docs/user/quick-start/#loading-an-image-into-your-cluster) our image into the `kind` Node.

```bash
$ kind load docker-image flask-test-gitops:latest --name cluster1

Image: "flask-test-gitops" with ID "sha256:c1fce423ff1acc2e1708156b1e35726dff6b1385917306682745753d54fe5352" not yet present on node "cluster1-control-plane", loading...
```

With the above command, the image is loaded onto the Node. We can now confirm that the image is there:

```bash
$ docker exec -it cluster1-control-plane crictl images

IMAGE                                      TAG                  IMAGE ID            SIZE
docker.io/kindest/kindnetd                 v20200725-4d6bea59   b77790820d015       119MB
docker.io/library/flask-test-gitops        latest               c1fce423ff1ac       909MB
docker.io/rancher/local-path-provisioner   v0.0.14              e422121c9c5f9       42MB
k8s.gcr.io/build-image/debian-base         v2.1.0               c7c6c86897b63       53.9MB
k8s.gcr.io/coredns                         1.7.0                bfe3a36ebd252       45.4MB
k8s.gcr.io/etcd                            3.4.13-0             0369cf4303ffd       255MB
k8s.gcr.io/kube-apiserver                  v1.19.1              8cba89a89aaa8       95MB
k8s.gcr.io/kube-controller-manager         v1.19.1              7dafbafe72c90       84.1MB
k8s.gcr.io/kube-proxy                      v1.19.1              47e289e332426       136MB
k8s.gcr.io/kube-scheduler                  v1.19.1              4d648fc900179       65.1MB
k8s.gcr.io/pause                           3.3                  0184c1613d929       686kB
```

With the image loaded, we can now make the relevant changes to our Deployment:

```yaml
# gitops-flask-kubernetes/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask
  namespace: test-namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flask
  template:
    metadata:
      labels:
        app: flask
    spec:
      containers:
      - name: flask
        image: docker.io/library/flask-test-gitops:latest # changed
        imagePullPolicy: Never # new
        ports:
        - containerPort: 5000
```

Note that we set the `imagePullPolicy` to `Never`. Why? This is because our image does not exist on `docker.io` and only exists locally. Apply the above changes by running `kubectl apply -f deployment.yaml`. Let's check if our Pods are running now:

```bash
$ kubectl get pods -n test-namespace

kubectl get pods -n test-namespace
NAME                     READY   STATUS        RESTARTS   AGE
flask-648fd69899-lrdpq   0/1     Terminating   0          10m
flask-6759dd97d8-69j4z   0/1     Terminating   0          40m
flask-687b44fb54-cxlmh   1/1     Running       0          6s
flask-687b44fb54-qgdxn   1/1     Running       0          4s
```

The older Pods are terminating and our Pods are finally running!

> It may take a minute or two before your Pods are in the `Running` state.

In the above Deployment we defined the `containerPort` to be 5000. We can port-forward the port and hit our application locally:

```bash
# Replace with your pod name
$ kubectl port-forward flask-687b44fb54-cxlmh 5000:5000 -n test-namespace

Forwarding from 127.0.0.1:5000 -> 5000
Forwarding from [::1]:5000 -> 5000

# In another terminal
$ curl localhost:5000

hello
```

In the above case we tested manually. Ideally, we need some [end-to-end tests](https://www.katalon.com/resources-center/blog/end-to-end-e2e-testing/) to verify if our application works as expected on the Kubernetes side. Let's write up some end-to-end tests to verify our application after it has been deployed to Kubernetes.

At the root of your project, add a new *test_end_to_end.py*:

```python
# gitops-flask-kubernetes/tests/test_end_to_end.py

import pytest
import requests


def test_index(endpoint):
    result = requests.get(f'http://{endpoint}/')
    assert b'hello' in result.content
```

Our test dictates that when we hit the endpoint, the HTTP response should include the word "hello". We're passing `endpoint` into our `test_index()` function as a fixture. Before we can run this new test, we need to define that fixture.

Create a new *conftest.py* file alongside the *test_end_to_end.py*:

```python
# gitops-flask-kubernetes/tests/conftest.py

from pytest import fixture


def pytest_addoption(parser):
    parser.addoption(
        "--endpoint",
        action="store"
    )


@fixture()
def endpoint(request):
    return request.config.getoption("--endpoint")
```

Here, we made the endpoint [configurable](https://docs.pytest.org/en/stable/example/simple.html#pass-different-values-to-a-test-function-depending-on-command-line-options), so we can essentially use the same test with different endpoints later in the course. Let's run the tests with `localhost:5000` for now.

> Make sure you still have port-forwarding running. If you exited, be sure to run the `kubectl port-forward` command again otherwise your test will fail.

```bash
$ pytest tests/test_end_to_end.py --endpoint localhost:5000

=============================== test session starts ===============================
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
rootdir: /Users/michael/repos/testdriven/gitops-flask-kubernetes
collected 1 item

tests/test_end_to_end.py .                                                  [100%]

================================ 1 passed in 0.09s ================================
```

## Summary

Let's wrap up here. In this chapter, we created a Docker image of our Flask application and loaded it into our Kind Node. We then confirmed that our application behaves as expected by port-forwarding into a Pod and getting a response.

Your project structure should now look like this:

```bash
├── Dockerfile
├── app.py
├── deployment.yaml
├── namespace.yaml
├── requirements.txt
└── tests
    ├── conftest.py
    ├── test_app.py
    ├── test_end_to_end.py
    └── test_smoke.py
```
