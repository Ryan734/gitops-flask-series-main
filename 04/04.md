# Application testing on Kubernetes and Networking

In this part, we'll explore how to configure a load balanced application hosted on Kubernetes. First, we'll create a [Kubernetes Service](https://kubernetes.io/docs/concepts/services-networking/service/) to expose our application as a network service. Then, we'll create a [Kubernetes Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) to route traffic coming from outside the cluster to our application running inside it. In particular, we'll leverage the [NGINX Ingress Controller](https://www.nginx.com/products/nginx-ingress-controller/) to do the load balancing and routing. As always, we'll use Dest-driven Development every step of the way.

> Curious about load balancing in general? Check out my other course, [Creating an HTTP Load Balancer in Python](https://testdriven.io/courses/http-load-balancer/), where you'll learn how to create a load balancer in Python using Test-driven Development.

## Kubernetes Service

In the previous chapter, we created a Deployment that spun up multiple Pods for our Flask application. Now, our goal is to create a load balancer that can route incoming traffic to either running Pod. From the client's perspective, it shouldn't matter which Pod serves the request -- the response should be the same.

When testing, we want to hit both Pods to confirm that they behave the same. We can create a Service that will load balance both Pods. A Service creates a [layer 4 load balancer](https://www.nginx.com/resources/glossary/layer-4-load-balancing/) that's able to direct requests to the right Pod via [iptables](https://en.wikipedia.org/wiki/Iptables) (or ipvs). Also, Kubernetes gives each Service an internal DNS name (accessible only from the Kubernetes cluster) that matches the pattern `|service_name|.|namespace|.svc.cluster.local`. For example, if we create a service called `flask` in the `test-namespace`, it will have the DNS entry `flask.test-namespace.svc.cluster.local`. We'll explore this more in a future chapter.

Let's create a Service for our Flask application. Create a new *service.yaml* file in your project root with the following code:

```yaml
# gitops_tutorial/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: flask
  namespace: test-namespace
spec:
  selector:
    app: flask
  ports:
    - protocol: TCP
      port: 5000
```

Pay attention to the `selector` field value -- it must match the [labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) we provided in the Deployment. This is how Kubernetes knows which Pods to send the request to.

Next, let's update our tests to include a Service test:

```python
# gitops_tutorial/tests/test_smoke.py

import yaml

from kubernetes import client, config

config.load_kube_config(context="kind-cluster1")


class TestSmoke:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()

    def test_create_namespace(self):
        with open("namespace.yaml") as f:
            namespace = yaml.safe_load(f)
        response = self.core_v1.create_namespace(body=namespace)
        result = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == result.metadata.name

    def test_create_deployment(self):
        with open("deployment.yaml") as f:
            deployment = yaml.safe_load(f)
        response = self.apps_v1.create_namespaced_deployment(body=deployment, namespace="test-namespace")
        result = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    # new
    def test_create_service(self):
        with open("service.yaml") as f:
            service = yaml.safe_load(f)
        response = self.core_v1.create_namespaced_service(body=service, namespace="test-namespace")
        result = self.core_v1.read_namespaced_service(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def teardown_class(self):
        self.core_v1.delete_namespace(name="test-namespace")
```

Similar to the Deployment tests, we used the `create_namespaced_service()` method to create our Service using the core v1 API.

We created the `test-namespace` Namespace manually in a previous step, so let's make sure we delete it before running the smoke tests.

```bash
$ kubectl delete ns test-namespace --context kind-cluster1
```

Let's run our tests:

```bash
(env)$ pytest tests/test_smoke.py

=============================== test session starts ===============================
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
rootdir: /Users/michael/repos/testdriven/gitops-flask-kubernetes
collected 3 items

tests/test_smoke.py ...                                                     [100%]

================================ 3 passed in 0.70s ================================
```

The tests pass but as mentioned earlier the Service is only accessible internally within the cluster. One way to access the Service would be to port-forward port 5000 and `curl` to see if our Service works. To do so, let's create a Namespace, Deployment and Service:

```bash
# Switch to the kind cluster
$ kubectl config use-context kind-cluster1

# Apply the Namespace, Deployment, and Service
$ kubectl apply -f namespace.yaml
$ kubectl apply -f deployment.yaml
$ kubectl apply -f service.yaml

# Confirm that the Pods are running
$ kubectl get pods -n test-namespace
NAME                     READY   STATUS    RESTARTS   AGE
flask-687b44fb54-5w2gt   1/1     Running   0          27s
flask-687b44fb54-wxpw7   1/1     Running   0          27s

# Check the details of the running service
$ kubectl get svc -n test-namespace
NAME    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
flask   ClusterIP   10.107.244.156   <none>        5000/TCP   2s

# Expose port 5000 in the Service to outside traffic
$ kubectl port-forward svc/flask -n test-namespace 5000:5000
Forwarding from 127.0.0.1:5000 -> 5000
Forwarding from [::1]:5000 -> 5000
```

Open another terminal tab or window and run the following:

```bash
$ curl localhost:5000
hello
```

We have confirmed that our Service is hitting our Pods and we can reach our application. When testing, we need to have a definitive endpoint to hit. To expose our Service to outside the cluster we need to create an Ingress resource.

## Kubernetes Ingress

An Ingress in Kubernetes is a networking component for exposing your Service outside of the cluster. We'll create one so we can expose our Flask application and run tests against it. To create an Ingress we need to deploy an Ingress controller. Ingress controllers act like reverse proxies and interact with the Kubernetes API server to dynamically update endpoints. We'll be using the [NGINX Ingress controller](https://kubernetes.github.io/ingress-nginx/how-it-works/) in this course.

Before we begin, we'll need to expose a few ports from our `kind` cluster to our host (laptop/PC). For this we'll need to recreate the `kind` cluster with a new configuration.

Start by deleting the existing cluster:

```bash
(env)$ kind delete cluster --name cluster1
Deleting cluster "cluster1" ...
```

Next, let's create a `kind` configuration file. Add a new *config.yaml* file to the project root with the following code:

```yaml
# gitops_tutorial/config.yaml

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
```

Finally, create the new cluster:

```bash
$ kind create cluster --name cluster1 --config=config.yaml

Creating cluster "cluster1" ...
 ‚úì Ensuring node image (kindest/node:v1.21.1) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to "kind-cluster1"
You can now use your cluster with:

kubectl cluster-info --context kind-cluster1

Have a nice day! üëã
```

In the above steps, we passed a custom configuration to [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/), which is used to bootstrap Kubernetes. We passed the label `ingress-ready=true` for all Nodes when they join and exposed port 80 and 443 to the host.

Let's check if the Node has the label:

```bash
$ kubectl get nodes --show-labels

NAME                     STATUS   ROLES                  AGE   VERSION   LABELS
cluster1-control-plane   Ready    control-plane,master   44s   v1.21.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress-ready=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=cluster1-control-plane,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=
```

In the above, we can see that the `ingress-ready=true` label is there.

Next, let's deploy the NGINX Ingress Controller using a [*deploy.yaml* file](https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml) provided by the NGINX controller maintainers.

```bash
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

namespace/ingress-nginx created
serviceaccount/ingress-nginx created
configmap/ingress-nginx-controller created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
service/ingress-nginx-controller-admission created
service/ingress-nginx-controller created
deployment.apps/ingress-nginx-controller created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
serviceaccount/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
```

Check if the Pods are running:

```bash
$ kubectl get pods -n ingress-nginx

NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-tj4vc        0/1     Completed   0          14m
ingress-nginx-admission-patch-vwq4b         0/1     Completed   0          14m
ingress-nginx-controller-55dccbb989-gb5db   1/1     Running     0          14m
```

> You might see the `ingress-nginx-controller` in the `ContainerCreating` state for a few seconds before it changes to `Running`.

We're now ready to create our Ingress.

Define the Ingress resource in a new *ingress.yaml* file:

```yaml
# gitops_tutorial/ingress.yaml

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: flask
  namespace: test-namespace
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: flask
          servicePort: 5000
```

In the above, we defined an Ingress resource and added ab [HTTP rule](https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules) where we reference the Flask Service's name and port as a backend in the spec.

Let's update our smoke tests:

```python
# gitops_tutorial/tests/test_smoke.py

import yaml

from kubernetes import client, config

config.load_kube_config(context="kind-cluster1")


class TestSmoke:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.networking_v1beta1 = client.NetworkingV1beta1Api() # new

    def test_create_namespace(self):
        with open("namespace.yaml") as f:
            namespace = yaml.safe_load(f)
        response = self.core_v1.create_namespace(body=namespace)
        result = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == result.metadata.name

    def test_create_deployment(self):
        with open("deployment.yaml") as f:
            deployment = yaml.safe_load(f)
        response = self.apps_v1.create_namespaced_deployment(body=deployment, namespace="test-namespace")
        result = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def test_create_service(self):
        with open("service.yaml") as f:
            service = yaml.safe_load(f)
        response = self.core_v1.create_namespaced_service(body=service, namespace="test-namespace")
        result = self.core_v1.read_namespaced_service(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    # new
    def test_create_ingress(self):
        with open("ingress.yaml") as f:
            ingress = yaml.safe_load(f)
        response = self.networking_v1beta1.create_namespaced_ingress(body=ingress, namespace="test-namespace")
        result = self.networking_v1beta1.read_namespaced_ingress(name="flask", namespace="test-namespace")
        assert "flask" == result.metadata.name
        assert "test-namespace" == result.metadata.namespace

    def teardown_class(self):
        self.core_v1.delete_namespace(name="test-namespace")
```

Here, we defined the [NetworkingV1beta1Api](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/NetworkingV1beta1Api.md) client and confirmed if our Ingress is able to be created.

Run the tests:

```bash
(env)$ pytest tests/test_smoke.py

=============================== test session starts ===============================
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
rootdir: /Users/michael/repos/testdriven/gitops-flask-kubernetes
collected 4 items

tests/test_smoke.py ....                                                    [100%]

================================ 4 passed in 0.85s ================================
```

They passed!

Now we can try to see if we can hit our application load balancer without using port forwarding on the Service.

## Testing the Ingress

To check if the Ingress works, let's first create all of our resources manually again and then we'll hit the endpoint:

```bash
$ kubectl apply -f namespace.yaml
namespace/test-namespace created

# Remember to load the image again because we recreated the cluster
$ kind load docker-image flask-test-gitops:latest --name cluster1

$ kubectl apply -f deployment.yaml
$ kubectl apply -f service.yaml
$ kubectl apply -f ingress.yaml

# Our endpoint is available on localhost:80 from any Host header
$ kubectl get ingress -n test-namespace
NAME    CLASS    HOSTS   ADDRESS     PORTS   AGE
flask   <none>   *       localhost   80      7m38s

# Hit the endpoint to test
$ curl localhost/
hello
```

Let's run our end-to-end tests using the `localhost` endpoint:

```bash
(env)$ pytest tests/test_end_to_end.py --endpoint localhost

=============================== test session starts ===============================
platform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
rootdir: /Users/michael/repos/testdriven/gitops-flask-kubernetes
collected 1 item

tests/test_end_to_end.py .                                                  [100%]

================================ 1 passed in 0.08s ================================
```

Finally, to attach the Ingress to a host, modify the Ingress resource to be exposed on a particular host header:

```yaml
# gitops_tutorial/ingress.yaml

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: flask
  namespace: test-namespace
spec:
  rules: # changed
  - host: apple.com
    http:
      paths:
      - path: /
        backend:
          serviceName: flask
          servicePort: 5000
```

In the above manifest, we addded a host header to allow us to route to our applications via host-based routing. Host-based routing allows us to reach an underlying service based on the `Host` header passed to the HTTP request.

> Why use host-based routing? In a real life cluster we can have an NGINX Ingress Controller host all of our Ingress endpoints. We can then distinguish between them via the `Host` header. We'll touch on this again in a future chapter.

Ingress resources allow us to specify multiple rules to direct to multiple services on the backend and `paths` allows us to specify path-based routing. For example, if we have `apple.com/payment` and `apple.com/messages` endpoints, we can take advantage of path-based routing to direct to different backend services, payment and messages, in this instance.

In the case above we simply left the path as `/`.

Update the Ingress and confirm if the host header works:

```bash
$ kubectl apply -f ingress.yaml

# Now our Ingress will only accept requests with the `apple.com` Host header
$ kubectl get ing -n test-namespace
NAME    CLASS    HOSTS           ADDRESS     PORTS   AGE
flask   <none>   apple.com   localhost   80      14m

# Test without the `apple.com` Host header
$ curl localhost/
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx</center>
</body>
</html>

# Test with the `www.apple.com` Host header
$ curl -H 'Host: apple.com' localhost/
hello
```

It works!

## Summary

In this chapter, you learned how to create a Service and an Ingress on a local kind cluster with an NGINX Ingress. Along the way, you also configured both path and host-based load balancing.

Your project structure should now look like this:

```bash
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ deployment.yaml
‚îú‚îÄ‚îÄ ingress.yaml
‚îú‚îÄ‚îÄ namespace.yaml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ service.yaml
‚îî‚îÄ‚îÄ tests
    ‚îú‚îÄ‚îÄ conftest.py
    ‚îú‚îÄ‚îÄ test_app.py
    ‚îú‚îÄ‚îÄ test_end_to_end.py
    ‚îî‚îÄ‚îÄ test_smoke.py
```

Cheers!
