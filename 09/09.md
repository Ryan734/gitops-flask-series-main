# Flask Microservices on Kubernetes and Cross Cluster Communication

The Microservices movement aims to break up a monolithic application into smaller services to increase uptime and resiliency. Often this adds complexity and maintenance overhead. Kubernetes is designed to support the microservice architecture with its networking features. In this section we are going deploy multiple microservices and explore cross cluster communication.

## Setting up Part 9

Create a new directory called `part9` inside the `gitops_tutorial` directory and copy file `test_end_to_end.py` and `chart` directory from part 8 and copy `Dockerfile` from part 3. We will be running commands from `gitops_tutorial/part9` and `gitops-series` directories. The starting directory structure should look like the following:

```bash
./gitops_tutorial/part9/
├── Dockerfile
├── chart
│   ├── Chart.yaml
│   ├── templates
│   │   ├── deployment.yaml
│   │   ├── ingress.yaml
│   │   └── service.yaml
│   └── values.yaml
└── tests
    └── test_end_to_end.py
```

## Changing our Flask application

Our Flask application we created in part 3 was a basic "hello world" application. We are now going to modify it to return the cluster it is in and the name of the service. Let's start off by writing a few tests.

```python
# gitops_tutorial/part9/tests/test_app.py

import pytest

from app import app


@pytest.fixture
def client():
    with app.test_client() as client:
        yield client


def test_hello(client):
    result = client.get('/')
    assert b'hello' in result.data


# new
def test_cluster_identity(client):
    result = client.get('/cluster')
    assert b'This is service1 in cluster cluster1' in result.data
``` 

Let's update our `app.py` to take in environment variables.

```python
# gitops_tutorial/part9/app.py

import os  # new
from flask import Flask 

app = Flask(__name__)

name = os.getenv("NAME", "service1")  # new
cluster_name = os.getenv("CLUSTER", "cluster1")  # new


@app.route('/')
def hello():
    return 'hello'


# new
@app.route('/cluster')
def cluster():
    return f'This is {name} in cluster {cluster_name}'


if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

Let's run our tests to see if they pass.

```
(env)$ pytest tests/test_app.py
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops-flask-series/part9
collected 2 items

test_app.py ..                                                                                                                                                   [100%]

==================================================================== 2 passed in 0.14s ====================================================================
```

We have added some new functionality to our application and we will need to update our end to end tests to reflect this.

```python
# gitops_tutorial/part9/tests/test_end_to_end.py

import re  # new
import pytest
import requests


def test_index(endpoint):
    result = requests.get(f'http://{endpoint}/')
    assert b'hello' in result.content


# new
def test_cluster(endpoint):
    result = requests.get(f'http://{endpoint}/cluster')
    data = result.content.decode()
    assert re.match(r'This is (\w+) in cluster (\w+)', data)
```

The test above will fail since we haven't deployed the `/cluster` endpoint yet. Let's run the test on cluster1.

```bash
$ pytest tests/test_end_to_end.py --endpoint localhost:80
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.7.3, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part9
plugins: requests-mock-1.7.0
collected 2 items

test_end_to_end.py .F                                                                                                                                           [100%]

============================================================================== FAILURES ===============================================================================
____________________________________________________________________________ test_cluster _____________________________________________________________________________

endpoint = 'localhost:80'

    def test_cluster(endpoint):
      result = requests.get("http://{}/cluster".format(endpoint))
>     data = result.data.decode()
E    AttributeError: 'Response' object has no attribute 'data'

test_end_to_end.py:11: AttributeError
========================================================================== warnings summary ===========================================================================
test_end_to_end.py:12
  /Users/neeran/Code/gitops-flask-series-testdriven/09/tests/test_end_to_end.py:12: DeprecationWarning: invalid escape sequence \w
    assert re.match('This is (\w+\s)in cluster (\w+)', data)

-- Docs: https://docs.pytest.org/en/latest/warnings.html
======================================================================= short test summary info =======================================================================
FAILED test_end_to_end.py::test_cluster - AttributeError: 'Response' object has no attribute 'data'
=============================================================== 1 failed, 1 passed, 1 warning in 0.86s ================================================================
```

Let's rebuild our Docker image (copied from part 3):

```
# gitops_tutorial/part9/Dockerfile
FROM python:alpine
RUN pip install flask
COPY app.py /app/app.py
CMD ["python", "/app/app.py"]
```

```sh
$ docker build -t flask-test-gitops:latest .
```

Confirm that it succeeds:

```
Successfully built 4d24b41ea180
Successfully tagged flask-test-gitops:latest
```

Let's not forget to move the image over to the `kind` Node if the cluster was recreated.

```sh
$ kind load docker-image flask-test-gitops:latest --name cluster1
```
We have now loaded the new image onto our Node. Since this image overrides the previous one we can simply recreate the Pods and make our test pass.

```bash
$ kubectl delete pods --all -n test-namespace
pod "flask-687b44fb54-4sqvr" deleted
pod "flask-687b44fb54-c5clz" deleted
# verify that the Pods are running again
$ kubectl get pods -n test-namespace
$ pytest tests/test_end_to_end.py --endpoint localhost:80
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.7.3, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops-flask-series-testdriven/09
plugins: requests-mock-1.7.0
collected 2 items

test_end_to_end.py ..                                                                                                                                           [100%]

========================================================================== 2 passed in 0.06s ==========================================================================
```
The test passes for cluster one with default values. We now need to make these values configurable as we will be deploying our application via Helm.

## Changing the Helm chart

We have a new requirement for our Kubernetes Deployment resource, it now needs to pass environment variables. Let's modify our `chart` directory in our `gitops-series` repository to reflect this and bump the version of the chart and appVersion.

```yaml
# gitops-series/chart/templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.name }}
  namespace: {{ .Values.namespace }}
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      app: {{ .Values.name }}
  template:
    metadata:
      labels:
        app: {{ .Values.name }}
    spec:
      containers:
      - name: {{ .Values.name }}
        image: {{ .Values.image }}
        imagePullPolicy: Never
        ports:
        - containerPort: 5000
        env:  # new
        - name: NAME
          value: {{ .Values.application_name }}
        - name: CLUSTER
          value: {{ .Values.application_cluster }}
```

```yaml
# gitops-series/chart/templates/ingress.yaml

{{ if .Values.ingress_enabled }}  # new
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: {{ .Values.name }}
  namespace: {{ .Values.namespace }}
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: {{ .Values.name }}
          servicePort: 5000
{{ end }}  # new
```

```yaml
# gitops-series/chart/Chart.yaml

apiVersion: v1
name: flask-application
version: 3  # changed
description: "Flask application"
sources:
  - https://github.com/paktek123/gitops-series
maintainers:
  - name: Neeran Gul
appVersion: 0.1.1
```

```yaml
# gitops-series/chart/values.yaml

name: flask-application
namespace: test-namespace
image: docker.io/library/flask-test-gitops:latest
replicas: 2
application_name: service1  # new
application_cluster: cluster1  # new
ingress_enabled: true  # new
```

Notice we change the Ingress to be optional now as we don't need it. The above can now be committed and pushed to the `gitops-series` repository.

## Setting up Microservices

We will now explore how Service networking works within Kubernetes. For this we will deploy the following "microservices":
- One Flask application in the same namespace as `test-namespace`
- One Flask application in a different namespace in the development cluster (cluster 1)
- One Flask application already exists in our staging cluster (cluster 2)

Before deploying, let's update our GitOps test to reflect this.

```python
# gitops_tutorial/part9/tests/test_gitops.py

from kubernetes import client, config
import pytest
import yaml
from conftest import option

config.load_kube_config(context=option.context)


class TestGitOps:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.networking_v1beta1 = client.NetworkingV1beta1Api()

    def test_flask_application_namespace(self):
        namespace = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == namespace.metadata.name

    def test_flask_application_deployment(self):
        deployment = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == deployment.metadata.name
        assert "test-namespace" == deployment.metadata.namespace

    def test_flask_application_service(self):
        service = self.core_v1.read_namespaced_service(name="flask", namespace="test-namespace")
        assert "flask" == service.metadata.name
        assert "test-namespace" == service.metadata.namespace

    def test_flask_application_ingress(self):
        ingress = self.networking_v1beta1.read_namespaced_ingress(name="flask", namespace="test-namespace")
        assert "flask" == ingress.metadata.name
        assert "test-namespace" == ingress.metadata.namespace


# new
@pytest.mark.skipif(option.context in ("kind-cluster2", "kind-cluster3"), reason="only deployed on development")
class TestGitOpsConditional:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
  
    def test_flask_internal_application_deployment(self):
        deployment = self.apps_v1.read_namespaced_deployment(name="flask-internal", namespace="test-namespace")
        assert "flask-internal" == deployment.metadata.name
        assert "test-namespace" == deployment.metadata.namespace
    
    def test_flask_internal_application_service(self):
        service = self.core_v1.read_namespaced_service(name="flask-internal", namespace="test-namespace")
        assert "flask-internal" == service.metadata.name
        assert "test-namespace" == service.metadata.namespace

    def test_flask_service2_application_namespace(self):
        namespace = self.core_v1.read_namespace(name="service2")
        assert "service2" == namespace.metadata.name

    def test_flask_service2_application_deployment(self):
        deployment = self.apps_v1.read_namespaced_deployment(name="flask-different-namespace", namespace="service2")
        assert "flask-different-namespace" == deployment.metadata.name
        assert "service2" == deployment.metadata.namespace

    def test_flask_service2_application_service(self):
        service = self.core_v1.read_namespaced_service(name="flask-different-namespace", namespace="service2")
        assert "flask-different-namespace" == service.metadata.name
        assert "service2" == service.metadata.namespace
```

Since our new `flask-internal` and `flask-different-namespace` Pods and our new `service2` Namespace are only deployed to the development cluster, we can skip running these tests for the staging and production clusters. Once the services are deployed we will try to reach them from our Flask application we deployed in part 8. Let's begin, create the following new files in your `gitops-series` repository:

```yaml
# gitops-series/development/service_internal/HelmRelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: flask-internal
  namespace: test-namespace
spec:
  interval: 5m
  releaseName: flask-internal
  chart:
    spec:
      chart: "./chart"
      version: "3"
      sourceRef:
        kind: GitRepository
        name: gitops-series
        namespace: flux-system
      interval: 1m
  values:
    name: flask-internal
    namespace: test-namespace
    application_name: internal
    application_cluster: development
    ingress_enabled: false
```

```yaml
# gitops-series/development/service_internal/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - HelmRelease.yaml
```

```yaml
# gitops-series/development/service2/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: service2
```

```yaml
# gitops-series/development/service2/HelmRelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: flask-different-namespace
  namespace: service2
spec:
  interval: 5m
  releaseName: flask-different-namespace
  chart:
    spec:
      chart: "./chart"
      version: "3"
      sourceRef:
        kind: GitRepository
        name: gitops-series
        namespace: flux-system
      interval: 1m
  values:
    name: flask-different-namespace
    namespace: service2
    application_name: service2
    application_cluster: development
    ingress_enabled: false
```

```yaml
# gitops-series/development/service2/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - HelmRelease.yaml
  - namespace.yaml
```

```yaml
# gitops-series/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base/ingress-nginx/
  - ./flask/
  - ./service_internal/
  - ./service2/
patchesStrategicMerge:
  - patches/ingress-nginx-deployment.yaml
  - patches/ingress-nginx-service.yaml
```

```yaml
# gitops-series/staging/flask-different-cluster/HelmRelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: flask-different-cluster
  namespace: test-namespace
spec:
  interval: 5m
  releaseName: flask-different-cluster
  chart:
    spec:
      chart: "./chart"
      version: "3"
      sourceRef:
        kind: GitRepository
        name: gitops-series
        namespace: flux-system
      interval: 1m
  values:
    name: flask-different-cluster
    namespace: test-namespace
    application_name: flask-different-cluster
    application_cluster: staging
    ingress_enabled: false
```

```yaml
# gitops-series/development/flask-different-cluster/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - HelmRelease.yaml
```

```yaml
# gitops-series/staging/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base/
  - ./flask-different-cluster/
patchesStrategicMerge:
  - patches/ingress-nginx-deployment.yaml
  - patches/ingress-nginx-service.yaml
```

Commit and push up the changes and wait for them to be deployed. Notice in the above we turn off the Ingress for the new services we are deploying. This is because we are not specifying a host for the Ingress which is already taken by the Ingress we created for the Flask application we deployed in earlier parts. Let's run our GitOps tests to see if everything was created as expected.

```sh
(env)$ pytest tests/test_gitops.py --context kind-cluster1
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part9
collected 9 items

test_gitops.py .........                                                                                                                                        [100%]

========================================================================== 9 passed in 0.61s ==========================================================================
```

If there are issues with the Helm releases then describe the `HelmRelease` object and fix any errors.

## Networking within Kubernetes

Let's `exec` into our Flask application to see if we can hit the `flask-internal` application and the `service2` application.

```sh
$ kubectl get pods -n test-namespace
NAME                              READY   STATUS    RESTARTS   AGE
flask-687b44fb54-52s4m            1/1     Running   0          23h
flask-687b44fb54-x8ksp            1/1     Running   0          23h
flask-internal-5db4756676-fsnjn   1/1     Running   0          11m
flask-internal-5db4756676-ng8rg   1/1     Running   0          11m
```

Copy the name of the first Pod from your terminal output (your Pod name will be different than the one shown above.) Run the following command to open a shell within the Pod:

```sh
# Replace <POD_NAME> with your Pod name
$ kubectl exec -it <POD_NAME> sh -n test-namespace --context kind-cluster1
```

Next, install the `curl` command inside the shell:

```
/ # apk add curl
```

Try the following commands and take note of the output:

```
/ # curl flask-internal:5000/cluster
This is internal in cluster development
/ # curl flask-internal.test-namespace.svc.cluster.local:5000/cluster
This is internal in cluster development
/ # curl flask-different-namespace.service2.svc.cluster.local:5000/cluster
This is service2 in cluster development
/ # cat /etc/resolv.conf
search test-namespace.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
```

Finally, exit the shell.

```
/ # exit
```

We're able to resolve the DNS of other Services in our Namespace and other Namespaces by using the `kube-dns` Service. Run the following command to see the details:

```sh
$ kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   7d
```

We are able to perform a HTTP request to the service and get back a response. The `/etc/resolv.conf` for all Pods is setup to resolve any services in their Namespace without the full domain due to the search domain checking the Namespace first.  

## Cross cluster communication

To be able to communicate with different Kubernetes clusters, we need to open up routing between them. In our case since all the clusters are listening locally on `localhost`, we will not be able to route out of the Pod itself. However, cross cluster communication is possible as long as the IP range of the destination Nodes is reachable from the source cluster Nodes. In our case we will use `ngrok` to expose our service to the internet and reach it directly. Head over to https://ngrok.com/download and download the latest `ngrok`.

After installing `ngrok`, run the following command:

```sh
$ ngrok http 81
```

Your terminal display should be replaced with the `ngrok` dashboard. Look for the line that describes how `ngrok` is forwarding HTTP requests. You should see something like the following:

```
Forwarding http://d8781dc5cde6.ngrok.io -> http://localhost:81
```

Copy the `ngrok` endpoint (e.g. `http://d8781dc5cde6.ngrok.io`).

Next, hit the `/cluster` path on the `ngrok` endpoint with `curl`:

```sh
# Replace <NGROK_ENDPOINT> with your ngrok endpoint
$ curl <NGROK_ENDPOINT>/cluster
This is flask in cluster staging
```

Try the same thing in the Pod shell. (Remember to use your own Pod name.)

```sh
# Replace <POD_NAME> appropriately
$ kubectl exec -it <POD_NAME> sh -n test-namespace --context kind-cluster1
```

Inside the shell, try the following:

```
/ # curl <NGROK_ENDPOINT>/cluster
This is flask in cluster staging
```

In the above we are able to reach our service since it is on the internet but here we can treat it as if its a routable endpoint from cluster 1. Say we wanted to give this service a name internally in cluster 1, we can use an `externalName` service.

Start by creating the following file:

```yaml
# gitops_tutorial/part9/external_service.yaml

apiVersion: v1
kind: Service
metadata:
  name: cluster2-service1
  namespace: test-namespace
spec:
  type: ExternalName
  externalName: d8781dc5cde6.ngrok.io
```

Then apply the file:

```sh
$ kubectl apply -f external_service.yaml
service/cluster2-service1 created
```

And go into the shell again:

```sh
$ kubectl exec -it <POD_NAME> sh -n test-namespace --context kind-cluster1
```

This time, copy the `ngrok` hostname, the endpoint without the `http://`. Inside the shell, run the following command, replacing the `<NGROK_HOSTNAME>` where applicable:

```
/ # curl -H 'Host: <NGROK_HOSTNAME>' cluster2-service1.test-namespace.svc.cluster.local/cluster
This is flask in cluster staging
```

We are able to use the `svc.cluster.local` endpoint to reach our service in cluster 2 from cluster 1. To tear down the `ngrok` tunnel simply `CTRL+C` the dashboard. 

> **IMPORTANT** 
> 
> We used `ngrok` as an example here, it should not be used in production to expose other Kubernetes clusters unless you know what you are doing. Normally some sort of VPC peering or networking peering can be done between the cluster subnets in your cloud provider.

## Directory structure

To see how the directory structure should look like, checkout the part9 branch on my [gitops-series](https://www.github.com/paktek123/gitops-series) git repository.

## Summary

In this part we learnt how to deploy multiple Flask based microservices on a Kubernetes cluster and learn how they are able to resolve and route each other using `kube-dns`. We also explored how we can allow cross cluster communication to take place by setting up an `ExternalName` service.

## Further reading

- Microservices vs Monolith: https://thenewstack.io/microservices-vs-monoliths-an-operational-comparison/
- DNS for Pods and Services: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
