# Deploying a Flask application with GitOps

In this section, we're going to use Flux v2 to deploy our Flask application. We're going to follow the same test-first approach from previous chapters -- write tests for what we expect to happen and then write code to make the tests pass. During the course of this chapter, we're also explore the GitRepository and Bucket CRDs and how the source controller can be used to sync our code hosted on them.

## Defining expectations

Our Flask application consists of a Namespace, a Deployment, a Service and an Ingress resource. After we deploy it via GitOps we should expect all these resources to be created. Let's write up some acceptance tests to confirm that our sync worked. From your project root, create a new `test_gitops.py` file with the following code:

```python
# gitops_tutorial/part6/tests/test_gitops.py

from kubernetes import client, config
import pytest
import yaml

config.load_kube_config(context="kind-cluster1")


class TestGitOps:
    def setup_class(self):
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.networking_v1beta1 = client.NetworkingV1beta1Api()

    def test_flask_application_namespace(self):
        namespace = self.core_v1.read_namespace(name="test-namespace")
        assert "test-namespace" == namespace.metadata.name

    def test_flask_application_deployment(self):
        deployment = self.apps_v1.read_namespaced_deployment(name="flask", namespace="test-namespace")
        assert "flask" == deployment.metadata.name
        assert "test-namespace" == deployment.metadata.namespace

    def test_flask_application_service(self):
        service = self.core_v1.read_namespaced_service(name="flask", namespace="test-namespace")
        assert "flask" == service.metadata.name
        assert "test-namespace" == service.metadata.namespace

    def test_flask_application_ingress(self):
        ingress = self.networking_v1beta1.read_namespaced_ingress(name="flask", namespace="test-namespace")
        assert "flask" == ingress.metadata.name
        assert "test-namespace" == ingress.metadata.namespace
```

Do these tests look familiar? This file is almost an exact copy of our smoke tests. The difference is that the GitOps tests only read the relevant resources and don't create them. Before you run the new test suite, make sure you destroy the old resources if you still have them running locally. Now, execute the following command:

```sh
(env)$ pytest tests/test_gitops.py
```

All four tests should fail because we haven't done the GitOps part yet.

```sh
FAILED test_gitops.py::TestGitOps::test_flask_application_namespace - kubernetes.client.exceptions.ApiException: (404)
FAILED test_gitops.py::TestGitOps::test_flask_application_deployment - kubernetes.client.exceptions.ApiException: (404)
FAILED test_gitops.py::TestGitOps::test_flask_application_service - kubernetes.client.exceptions.ApiException: (404)
FAILED test_gitops.py::TestGitOps::test_flask_application_ingress - kubernetes.client.exceptions.ApiException: (404)
```

## Setup Deployment

We first need to create a Git repository that will host our Kubernetes resources. Follow GitHub's [documentation](https://docs.github.com/en/github/getting-started-with-github/create-a-repo) to create a new repository called `gitops-series` in your personal account. [Clone the repo](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository) to your local machine in whatever directory you store your projects. Open that new repository in your favorite IDE or navigate to it in your terminal and add the following files:

```yaml
# gitops-series/namespace.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: test-namespace
```

```yaml
# gitops-series/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask
  namespace: test-namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flask
  template:
    metadata:
      labels:
        app: flask
    spec:
      containers:
      - name: flask
        image: docker.io/library/flask-test-gitops:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 5000
```

Note that we reference `docker.io/library/flask-test-gitops:latest` docker image. If you have recreated the cluster please go back to part 3 and rebuild the docker image. Then run the following command to load it into the kind cluster.

```bash
$ kind load docker-image flask-test-gitops:latest --name cluster1
```

```yaml
# gitops-series/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: flask
  namespace: test-namespace
spec:
  selector:
    app: flask
  ports:
    - protocol: TCP
      port: 5000
```

```yaml
# gitops-series/ingress.yaml

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: flask
  namespace: test-namespace
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: flask
          servicePort: 5000
```

```yaml
# gitops-series/kustomization.yaml

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - ingress.yaml
  - namespace.yaml
  - service.yaml
```

These are the files we created in Part 4 with `kustomization.yaml`. Take note of the `resources` key in which we explicitly state which manifests are going to be applied. Your `gitops-series` directory structure should look like this:

```sh
├── deployment.yaml
├── ingress.yaml
├── kustomization.yaml
├── namespace.yaml
└── service.yaml
```

Next, add all of the new files, commit them, and push them to the remote repo.

```sh
$ git add .
$ git commit -m 'Adding first flask app iteration'
$ git push
```

## Deploying via Git Repository

Now we have to install Flux such that it points to our repository. In part 5 we installed Flux v2 on a running `kind` cluster. If the cluster is still running we can use the same Flux to add a new GitRepository CRD. If the cluster does not exist please follow steps in part 5 to recreate the cluster and install Flux v2. Let's have a look at state of our GitRepository CRDs.

```sh
$ kubectl get gitrepository -n flux-system
NAME          URL                                          READY   STATUS                                                              AGE
flux-system   ssh://git@github.com/paktek123/fleet-infra   True    Fetched revision: main/33f5841d64fd7823c175099e07cba1abd6304da6     2d12h
podinfo       https://github.com/stefanprodan/podinfo      True    Fetched revision: master/ef98a040c89180a4f39c0ab01dac47e6c3fced08   2d11h
```

In the above output `flux-system` is from when we deployed Flux v2 in part 5 and `podinfo` is the sample Git repository from part 5. We are now going to create a new one for our `gitops-series` git repository.

Note: If the git repository was created via www.github.com then the branch name will be `main`. If the gitops-series git repository was created via other means then the branch name will be `master`. For consistency purposes I would recommend to branch off `master` and create a `main` branch.

```yaml
# gitops_tutorial/part6/gitrepository.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: gitops-series
  namespace: flux-system
spec:
  interval: 30s
  ref:
    branch: main
  # replace with your username
  url: https://github.com/paktek123/gitops-series
```

Let's apply the above by running `kubectl apply -f gitrepository.yaml`.

```sh
$ kubectl get gitrepository -n flux-system
NAME            URL                                          READY   STATUS                                                              AGE
flux-system     ssh://git@github.com/paktek123/fleet-infra   True    Fetched revision: main/33f5841d64fd7823c175099e07cba1abd6304da6     2d12h
gitops-series   https://github.com/paktek123/gitops-series   True    Fetched revision: main/a1a80bf1dbe39535e06fbfc542d34737587066b8     56s
podinfo         https://github.com/stefanprodan/podinfo      True    Fetched revision: master/ef98a040c89180a4f39c0ab01dac47e6c3fced08   2d12h
```

We now have our Git Source defined. To setup Continoues Deployment we need to create a Kustomization CRD. Let's create one. Run `kubectl apply -f kustomization_flask_application.yaml` on the file below.

```yaml
# gitops_tutorial/part6/kustomization_flask_application.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flask-application
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./
  prune: true
  sourceRef:
    kind: GitRepository
    name: gitops-series
  validation: client
```

In the above we define a Kustomization CRD which references our `kustomization.yaml` file we pushed to our `gitops-series` git repository. The sync interval is 5 minutes. To check if the `Kustomization` is working as expect let's run `kubectl get kustomization -n flux-system`. 


```sh
$ kubectl get kustomization -n flux-system
NAME                READY   STATUS                                                              AGE
flask-application   True    Applied revision: main/52fd3410695922d4f3734ff23c5f2f113932f533     5m35s
flux-system         True    Applied revision: main/2526672f1cbff850f683f496867670912e604adb     7m57s
podinfo             True    Applied revision: master/627d5c4bb67b77185f37e31d734b085019ff2951   11s
```

We can see that the kustomization is ready and applied our version. Let's run our smoke tests to confirm if all the resources we expected have been created.

```sh
(env)$ pytest tests/test_gitops.py
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part6
collected 4 items

test_gitops.py ....                                                                                                                                             [100%]

========================================================================== 4 passed in 1.01s ==========================================================================
```

The tests pass! This means that our application deployed as expected. However to reliably run our end-to-end tests we will need to deploy NGINX Ingress via GitOps. More on that in part 7.

## Deploying via S3 Bucket

As discussed in Part 5, the source controller allows us to deploy via an S3 bucket. Why would one opt to use S3 as an artifact store? Surely code is better situated in a VCS (Version Control System) like Github? True, code is better in a VCS but an there are many scenarios where we need to use an S3 bucket. One scenario can be due to scaling. For example we have 50+ Kubernetes clusters running each with its own Flux deployed. Each Flux polls multiple repositories every 5 minutes causing a DDOS on a self hosted VCS like Github Enterprise or Gitlab EE. To avoid that a pipeline can be setup to copy all Git Repository into S3, then point all the Fluxes to the S3 repository. This way we can keep the VCS intact as they are vital to an engineering organisation.

Another example can be using S3 as a Git Repository. AWS S3 allows multiple [versions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html) of the same object hence it can act like a VCS same as any git server. One can use libraries like [JGit](https://github.com/eclipse/jgit) to host a git server backed by S3. In this case the git server and the S3 bucket are one in the same and pointing to the S3 bucket has the same effect. Security can play another part as many Git servers do not have granular controls on repository paths similar to AWS S3. This can significantly reduce the surface area of the attack as the Git server cannot to reached from the Kubernetes cluster.

### Deploying localstack

In this section we will push our YAML manifests to a [localstack](https://github.com/localstack/localstack) S3 Bucket and setup a continuous delivery pipeline via GitOps. Localstack is a local development environment for AWS that supports a many services including S3, we can use this to host our YAML manifests as opposed to a Git repository in the previous section. Let's deploy a localstack instance via [docker-compose](https://docs.docker.com/compose/install). First head over to the [installation page](https://docs.docker.com/compose/install/) and install docker-compose for your OS. Then create this `docker-compose.yaml` in the gitops-tutorial directory.

```yaml
# gitops_tutorial/part6/docker-compose.yaml
version: '3.9'
services:
  localstack:
    image: localstack/localstack:latest
    container_name: localstack
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3
    volumes:
      - '/var/run/docker.sock:/var/run/docker.sock'
```
In the above we define the localstack container and expose port 4566. In the environment variables we pass `SERVICES=s3` because localstack runs many AWS services, we are only interested in S3 and finally we mount expose our locally running docker socket to the container, which is a requirement for localstack. Let's spin up our Docker container.

```bash
$ docker-compose up -d
# check if the container is running
$ docker ps
CONTAINER ID   IMAGE                          COMMAND                  CREATED        STATUS        PORTS                                                      NAMES
1ab7b2ff21e1   localstack/localstack:latest   "docker-entrypoint.sh"   5 seconds ago   Up 3 seconds   4571/tcp, 0.0.0.0:4566->4566/tcp, 8080/tcp   localstack
f6a5824b4e46   kindest/node:v1.19.1           "/usr/local/bin/entr…"   12 days ago    Up 12 days    127.0.0.1:60974->6443/tcp                                  cluster1-control-plane
```
We can see that our localstack container is up and running. To interact with the container we need an S3 compatible client. We will install the AWS CLI to interact with our local S3 Bucket.

### Setting up the AWS CLI

```bash
$ pip install awscliv2
...
$ awsv2 --version
...
aws-cli/2.1.36 Python/3.8.8 Linux/4.19.121-linuxkit docker/x86_64.amzn.2 prompt/off
```
If this is the first time you are using AWS CLI, it will prompt you to fill in some fields. Since we are not interacting with a real AWS environment any values can be added. If you have used AWS CLI before I recommend to create a new profile called `default` with similar section in your `~/.aws/credentials` to the below.

```ini
[default]
aws_access_key_id     = FAKE
aws_secret_access_key = FAKE
```
With our credentials setup let's try to list some S3 buckets. Take note that localstack is exposed on `0.0.0.0:4566` meaning that we can hit it on our local IP or `127.0.0.1` (localhost). In this case we will hit on localhost.

Note: In some cases localhost might not work, please use your own local IP you can get it [here](https://www.whatismybrowser.com/detect/what-is-my-local-ip-address).

```bash
$ awsv2 --endpoint-url=http://localhost:4566 s3 ls
```
The output of the above command is empty since we haven't create any S3 bucket. Let's create one.
```bash
$ awsv2 --endpoint-url=http://localhost:4566 s3 mb s3://example
make_bucket: example
$ awsv2 --endpoint-url=http://localhost:4566 s3 ls
2021-04-03 16:06:32 example
```
We have now create our S3 bucket. Let's copy the manifests for our flask application into the S3 bucket.

### Setting up the S3 CD pipeline via the Bucket CRD

```bash
$ cd gitops-series
$ awsv2 --endpoint-url=http://localhost:4566 s3 sync . s3://example --exclude "*" --include "*.yaml"
upload: ./deployment.yaml to s3://example/deployment.yaml
upload: ./ingress.yaml to s3://example/ingress.yaml
upload: ./kustomization.yaml to s3://example/kustomization.yaml
upload: ./namespace.yaml to s3://example/namespace.yaml
upload: ./service.yaml to s3://example/service.yaml
```
In the above command we only include files with yaml extension and exclude any other extensions. Our bucket is now ready to deploy. We now need to create the `Bucket` CRD.

```yaml
# gitops_tutorial/part6/bucket.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: Bucket
metadata:
  name: flask
  namespace: flux-system
spec:
  interval: 5m
  provider: generic
  bucketName: example
  endpoint: <yourlocalip>:4566
  insecure: true
  secretRef:
    name: s3-credentials
---
apiVersion: v1
kind: Secret
metadata:
  name: s3-credentials
  namespace: flux-system
type: Opaque
data:
  accesskey: RkFLRQ==
  secretkey: RkFLRQ==
```
In the above CRD, note that for the endpoint we use our local IP address. This is because when we apply the CRD the source controller will try to talk to `localhost` or `127.0.0.1`. These two addresses cannot route outside of the docker container that the source controller runs on, in other words the source controller will end up trying to reach a locally running localstack service and will get a connection refused. There are multiple ways of getting your local IP address depending on your OS, have a look [here](https://www.whatismybrowser.com/detect/what-is-my-local-ip-address).

We reference a secret resource which holds credentials for our S3 bucket. The accesskey and secretkey are base64 of the `FAKE` credentials we setup above generated by running the `base64` command. Let's try listing the objects with our local IP address now and create the bucket CRD.

```bash
# replace with your local IP address
$ awsv2 --endpoint-url=http://192.168.0.11:4566 s3 ls s3://example
2021-04-03 16:27:19        396 deployment.yaml
2021-04-03 16:27:19        232 ingress.yaml
2021-04-03 16:27:19        148 kustomization.yaml
2021-04-03 16:27:19         64 namespace.yaml
2021-04-03 16:27:20        160 service.yaml
# apply the bucket manifest
$ kubectl apply -f bucket.yaml
bucket.source.toolkit.fluxcd.io/flask created
secret/s3-credentials created
$ kubectl get bucket -n flux-system
NAME    URL   READY   STATUS                                                       AGE
flask         True    Fetched revision: cd6d0c5de2318cb3ff36136b0944dd6175243567   16s
```
Our source is ready to use. Let's reference in our kustomization CRD to setup our CD pipeline.

```yaml
# gitops_tutorial/part6/kustomization_flask_application_bucket.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flask-application
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./
  prune: true
  sourceRef:
    kind: Bucket
    name: flask
  validation: client
```
The above CRD looks very similar to our kustomization CRD we created in the previous section. The only thing that changed was the sourceRef as we now host our code in the S3 bucket. Let's delete the kustomization we created in the previous section and delete all remnants since we are deploying the same application to avoid a clash.

```bash
$ kubectl delete kustomization flask-application -n flux-system
$ kubectl delete ns test-namespace
```
Now we are ready to see if our kustomization works. Let's apply our Bucket manifest.

```bash
$ kubectl apply -f kustomization_flask_application_bucket.yaml
kustomization.kustomize.toolkit.fluxcd.io/flask-application created
$ kubectl get kustomization  -n flux-system
NAME                READY   STATUS                                                              AGE
flask-application   True    Applied revision: cd6d0c5de2318cb3ff36136b0944dd6175243567          2m8s
flux-system         True    Applied revision: main/0ff9e029863d8cc65adbc09ad2a69ce50209c1ea     12d
podinfo             True    Applied revision: master/ef98a040c89180a4f39c0ab01dac47e6c3fced08   12d
$ kubectl get pods  -n test-namespace
NAME                     READY   STATUS    RESTARTS   AGE
flask-687b44fb54-94m4d   1/1     Running   0          5m10s
flask-687b44fb54-wkgts   1/1     Running   0          5m10s
```
Our application is running as we expect. Let's run our tests verify everything was created.

```bash
(env)$ pytest tests/test_gitops.py
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.3, pytest-6.2.1, py-1.10.0, pluggy-0.13.1
rootdir: /Users/neeran/Code/gitops_tutorial/part6
collected 4 items

test_gitops.py ....                                                                                                                                             [100%]

========================================================================== 4 passed in 1.01s ==========================================================================
```
The tests pass! We have successfully deployed our application via an S3 bucket!

### Using AWS CLI to deploy to S3

To copy our Kubernetes manifests to a real S3 bucket in an AWS Account, we can use the AWS CLI but we do not need to specify the `--endpoint` argument and we need to [generate credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html). Instead of the `FAKE` values we put we will configure our AWS CLI with the IAM Access Key in `~/.aws/credentials` and run the following command to copy our manifests.

```bash
$ awsv2 s3 sync . s3://example --exclude "*" --include "*.yaml"
```

To setup the Flux side it is recommended to follow the CRDs and IAM policies defined [here](https://toolkit.fluxcd.io/components/source/buckets/#aws-iam-authentication).

## Directory structure

To see how the directory structure should look like, checkout the part6 branch on my [gitops-series](https://www.github.com/paktek123/gitops-series) git repository.

## Summary

We have deployed our Flask application via GitOps and two different sources, a Git repository and an S3 bucket. We then confirmed it works by running tests against what we expected in our GitOps repository and end to end tests.
